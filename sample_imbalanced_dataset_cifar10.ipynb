{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7709155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip\n",
      "  Downloading https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip\n",
      "     - 297 kB 1.7 MB/s   \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3 in ./myenv/lib/python3.6/site-packages (from torchsampler==0.1.1) (1.10.0)\n",
      "Requirement already satisfied: torchvision>=0.5 in ./myenv/lib/python3.6/site-packages (from torchsampler==0.1.1) (0.11.1)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.6/site-packages (from torchsampler==0.1.1) (1.1.5)\n",
      "Requirement already satisfied: typing-extensions in ./myenv/lib/python3.6/site-packages (from torch>=1.3->torchsampler==0.1.1) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses in ./myenv/lib/python3.6/site-packages (from torch>=1.3->torchsampler==0.1.1) (0.8)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.6/site-packages (from torchvision>=0.5->torchsampler==0.1.1) (1.19.5)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in ./myenv/lib/python3.6/site-packages (from torchvision>=0.5->torchsampler==0.1.1) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./myenv/lib/python3.6/site-packages (from pandas->torchsampler==0.1.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./myenv/lib/python3.6/site-packages (from pandas->torchsampler==0.1.1) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->torchsampler==0.1.1) (1.16.0)\n",
      "Building wheels for collected packages: torchsampler\n",
      "  Building wheel for torchsampler (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchsampler: filename=torchsampler-0.1.1-py3-none-any.whl size=3839 sha256=3c9eaaec1aca6bc68c6bd1e639c1cccbf65393facb12dad42b283124307e6b06\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ba7souz5/wheels/68/a0/61/e9306c707e1c2b6c588122f001efcb2f11bdb459ab7bbfe5dc\n",
      "Successfully built torchsampler\n",
      "Installing collected packages: torchsampler\n",
      "Successfully installed torchsampler-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfd7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import argparse\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def seed_everything(seed=12):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "parser = argparse.ArgumentParser(description='BalancedLSF Training')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
    "parser.add_argument('--batch_size', default=50, type=int, help='batch size')\n",
    "parser.add_argument('--test_batch_size', default=2048, type=int, help='batch size')\n",
    "parser.add_argument('--num_epoch', default=100, type=int, help='epoch number')\n",
    "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
    "parser.add_argument('--imb_factor', default=0.01, type=float, help='Imbalanced factor')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc708434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMBALANCECIFAR10(torchvision.datasets.CIFAR10):\n",
    "    cls_num = 10\n",
    "\n",
    "    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)\n",
    "        np.random.seed(rand_number)\n",
    "        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n",
    "        self.gen_imbalanced_data(img_num_list)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.targets\n",
    "\n",
    "    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n",
    "        img_max = len(self.data) / cls_num\n",
    "        img_num_per_cls = []\n",
    "        if imb_type == 'exp':\n",
    "            for cls_idx in range(cls_num):\n",
    "                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "                img_num_per_cls.append(int(num))\n",
    "        elif imb_type == 'step':\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max))\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max * imb_factor))\n",
    "        else:\n",
    "            img_num_per_cls.extend([int(img_max)] * cls_num)\n",
    "        return img_num_per_cls\n",
    "\n",
    "    def gen_imbalanced_data(self, img_num_per_cls):\n",
    "        new_data = []\n",
    "        new_targets = []\n",
    "        targets_np = np.array(self.targets, dtype=np.int64)\n",
    "        classes = np.unique(targets_np)\n",
    "        self.num_per_cls_dict = dict()\n",
    "        for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
    "            self.num_per_cls_dict[the_class] = the_img_num\n",
    "            idx = np.where(targets_np == the_class)[0]\n",
    "            np.random.shuffle(idx)\n",
    "            selec_idx = idx[:the_img_num]\n",
    "            new_data.append(self.data[selec_idx, ...])\n",
    "            new_targets.extend([the_class, ] * the_img_num)\n",
    "        new_data = np.vstack(new_data)\n",
    "        self.data = new_data\n",
    "        self.targets = new_targets\n",
    "        \n",
    "    def get_cls_num_list(self):\n",
    "        cls_num_list = []\n",
    "        for i in range(self.cls_num):\n",
    "            cls_num_list.append(self.num_per_cls_dict[i])\n",
    "        return cls_num_list\n",
    "\n",
    "class IMBALANCECIFAR100(IMBALANCECIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the `CIFAR10` Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'meta',\n",
    "        'key': 'fine_label_names',\n",
    "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
    "    }\n",
    "    cls_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418f6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b35ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41e00ad1c334b469419683bead18a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Class frequency in Train Dataset [5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "Standard IID Batch Sampler: [22, 9, 10, 2, 0, 4, 0, 2, 1, 0]\n",
      "Imbalanced Dataset Sampler: [8, 5, 5, 4, 6, 5, 7, 3, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "from torchsampler import ImbalancedDatasetSampler\n",
    "seed_everything()\n",
    "mean_cifar10, std_cifar10 = (0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)\n",
    "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
    "            transforms.Normalize(mean_cifar10, std_cifar10), ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10, std_cifar10),])\n",
    "#train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_dataset = IMBALANCECIFAR10(root='./data', imb_type='exp', imb_factor=args.imb_factor, rand_number=0, train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2048, shuffle=False, num_workers=4)\n",
    "\n",
    "train_loader_bal = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, num_workers=4, sampler=ImbalancedDatasetSampler(train_dataset))\n",
    "test_loader_bal = torch.utils.data.DataLoader(test_dataset, batch_size=2048, shuffle=False, num_workers=4)\n",
    "\n",
    "print('Class frequency in Train Dataset',train_dataset.get_cls_num_list())\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    cls_batch_sampler = [(targets==i).sum().item() for i in range(10)]\n",
    "    print('Standard IID Batch Sampler:',cls_batch_sampler)\n",
    "    break\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader_bal):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    cls_batch_sampler = [(targets==i).sum().item() for i in range(10)]\n",
    "    print('Imbalanced Dataset Sampler:',cls_batch_sampler)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883a752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
